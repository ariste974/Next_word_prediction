{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c52d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b6a5a-7215-42b2-a11f-c7884e97304e",
   "metadata": {},
   "source": [
    "Ariste Mathiot, Nicolas Marchand, Neil Mahcer DIA5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02472fd2",
   "metadata": {},
   "source": [
    "# Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d39fe8-bd82-4023-bf72-89ae37c62d63",
   "metadata": {},
   "source": [
    "Dans ce projet, nous avons développé un modèle de prédiction de texte capable de suggérer le prochain mot d'une phrase. Nous avons comparé différentes approches, allant des LSTM classiques aux Transformers modernes, jusqu’à des modèles pré-entraînés comme DistilGPT2, afin d’évaluer leur efficacité sur une tâche de complétion de texte.\n",
    "\n",
    "Le dataset « Sherlock Holmes Book Text Data » contient le texte complet des histoires de détective écrites par Sir Arthur Conan Doyle mettant en scène Sherlock Holmes. Ce jeu de données est conçu pour des tâches de prédiction de texte et de modélisation du mot suivant.\n",
    "\n",
    "Il comprend le texte original des récits de Sherlock Holmes, permettant aux utilisateurs d’analyser les structures linguistiques et de développer des algorithmes capables de prédire le mot suivant d’une phrase en fonction du contexte fourni par le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Read the text file\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba549e3",
   "metadata": {},
   "source": [
    "Creation du dataset:\n",
    "- Transformer le text en sequence \n",
    "- Utliser la technique du n_gram pour qu'il puisse mieux comprendre le sens des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ac8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "a=1\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c960d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f22aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words, max_sequence_len, X.shape, y.shape, input_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042f8f7-3fc1-4cbb-bbaf-9433ca6c40fe",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa3b91",
   "metadata": {},
   "source": [
    "# Utilisation d'un RNN tout simple avec du early stopping sur le val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aca3c8-ff13-4e13-88f3-1b75b332c948",
   "metadata": {},
   "source": [
    "Les RNN sont des réseaux conçus pour traiter des données séquentielles (texte, séries temporelles, audio...) où chaque sortie dépend des entrées précédentes. Ils \"mémorisent\" une information d’état via des poids récurrents, qui évoluent au fil des étapes de la séquence.\n",
    "\n",
    "Problème de vanishing gradients sur les longues séquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import SimpleRNN as RNN\n",
    "model_RNN = Sequential()\n",
    "model_RNN.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model_RNN.add(RNN(150))\n",
    "model_RNN.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Construire le modèle\n",
    "model_RNN.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveiller la perte pendant l'entraînement\n",
    "    patience=5,      # Arrêter si la perte ne s'améliore pas après 3 époques\n",
    "    restore_best_weights=True  # Restaurer les poids du meilleur modèle\n",
    ")\n",
    "# Afficher le résumé\n",
    "print(model_RNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_RNN=model_RNN.fit(X_train, y_train, epochs=100, batch_size=32 ,verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping])\n",
    "model_RNN.save(\"modelRNN.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883c185",
   "metadata": {},
   "source": [
    "Il s'arrete à 7 epochs ce qui me parait peu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a076fe",
   "metadata": {},
   "source": [
    "# Utilisation du dropout pour qu'il puisse etre plus general et ne pas overfitt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070ee8c-2399-4076-9bd6-7ea7b6678763",
   "metadata": {},
   "source": [
    "Cela rend l’apprentissage plus lent (et donc une accuracy plus basse au début), mais le modèle généralise mieux sur les données jamais vues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Définition du modèle avec des couches supplémentaires et du Dropout\n",
    "model_RNN_drop = Sequential()\n",
    "model_RNN_drop.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model_RNN_drop.add(RNN(150, return_sequences=True))  # LSTM avec retour des séquences\n",
    "model_RNN_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_RNN_drop.add(RNN(100))  # Deuxième couche LSTM\n",
    "model_RNN_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_RNN_drop.add(Dense(128, activation='relu'))  # Couche Dense intermédiaire\n",
    "model_RNN_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_RNN_drop.add(Dense(total_words, activation='softmax'))  # Couche de sortie\n",
    "\n",
    "# Construire le modèle\n",
    "model_RNN_drop.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "# Afficher le résumé\n",
    "print(model_RNN_drop.summary())\n",
    "\n",
    "# Compilation du modèle\n",
    "model_RNN_drop.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Configuration de l'Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveiller la perte pendant l'entraînement\n",
    "    patience=5,      # Arrêter si la perte ne s'améliore pas après 3 époques\n",
    "    restore_best_weights=True  # Restaurer les poids du meilleur modèle\n",
    ")\n",
    "\n",
    "# Entraînement du modèle avec Early Stopping\n",
    "history_RNN_drop=model_RNN_drop.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_test,y_test) ,verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "model_RNN_drop.save(\"modelRNN_dropout.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c538834-72a7-408f-98c4-d0b54b85a508",
   "metadata": {},
   "source": [
    "Donc, d'après les metrics d'evaluation, le modèle sans le dropout est meilleur. Mais, je pense que s'il on laissait le modele avec dropout plus s'entrainer on obtiendrait des meilleurs résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef6ac8",
   "metadata": {},
   "source": [
    "# LSTM Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2291352-f91b-4577-858c-8f2ce5e89065",
   "metadata": {},
   "source": [
    "Les LSTM sont une version améliorée des RNN, conçue pour résoudre le problème de mémoire courte. \n",
    "Grâce à des portes (input, forget, output), ils gèrent mieux quelles informations garder ou oublier dans la séquence.\n",
    "\n",
    "Regle le problème du vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e328db",
   "metadata": {},
   "source": [
    "J'ai essayé de changer le batch size.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Le batch size à 32 est mieux qu'à 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc01bf6",
   "metadata": {},
   "source": [
    "Pour le early stopping il s'arrete à 8 mais je pense qu'il peut apprendre un peu plus donc j'ai augmenté la patience dans le early stopping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d12591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model_earlystopping = Sequential()\n",
    "model_earlystopping.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model_earlystopping.add(LSTM(150))\n",
    "model_earlystopping.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Construire le modèle\n",
    "model_earlystopping.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveiller la perte pendant l'entraînement\n",
    "    patience=5,      # Arrêter si la perte ne s'améliore pas après 3 époques\n",
    "    restore_best_weights=True  # Restaurer les poids du meilleur modèle\n",
    ")\n",
    "# Afficher le résumé\n",
    "print(model_earlystopping.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc39ea-39d1-4874-9074-62ebd35ac36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_earlystopping.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_earlystopping=model_earlystopping.fit(X_train, y_train, epochs=100, batch_size=32 ,verbose=1,validation_data=(X_test,y_test), callbacks=[early_stopping])\n",
    "model.save(\"modelLSMTsimple_earlystopping.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee24ff-9bbb-4afb-8e83-76d62b0c656a",
   "metadata": {},
   "source": [
    "J'ai decidé de faire les 100 epochs pour visualiser si c'est juste le début ou le modele s'améliore à un moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ed89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Construire le modèle\n",
    "model.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "# Afficher le résumé\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history=model.fit(X_train, y_train, epochs=100, batch_size=32 ,verbose=1,validation_data=(X_test,y_test))\n",
    "model.save(\"modelLSMTsimple.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a8011-e7cb-47e9-8353-d9b5ab024686",
   "metadata": {},
   "source": [
    "Ici grâce au graphique on voit bien que le meilleur epoch est à 5 ou 6 sinon ça devient de l'overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee03f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], linestyle='--', label=f'Train Acc ')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411fa5b0",
   "metadata": {},
   "source": [
    "# Rajout du dropout LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d81554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Définition du modèle avec des couches supplémentaires et du Dropout\n",
    "model_drop = Sequential()\n",
    "model_drop.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model_drop.add(LSTM(150, return_sequences=True))  # LSTM avec retour des séquences\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(LSTM(100))  # Deuxième couche LSTM\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(Dense(128, activation='relu'))  # Couche Dense intermédiaire\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(Dense(total_words, activation='softmax'))  # Couche de sortie\n",
    "\n",
    "# Construire le modèle\n",
    "model_drop.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "# Afficher le résumé\n",
    "print(model_drop.summary())\n",
    "\n",
    "# Compilation du modèle\n",
    "model_drop.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Configuration de l'Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveiller la perte pendant l'entraînement\n",
    "    patience=5,      # Arrêter si la perte ne s'améliore pas après 3 époques\n",
    "    restore_best_weights=True  # Restaurer les poids du meilleur modèle\n",
    ")\n",
    "\n",
    "# Entraînement du modèle avec Early Stopping\n",
    "history_drop=model_drop.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_test,y_test) ,verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "model_drop.save(\"modelLSMT_dropout.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673398f4-7d4d-4131-8aca-a3954b7d883a",
   "metadata": {},
   "source": [
    "Ici, le meilleur modèle est celui avec le dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1a4f5-8e03-4872-8946-f4de809a5b97",
   "metadata": {},
   "source": [
    "# Reflexion sur la metrics d'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bc992",
   "metadata": {},
   "source": [
    "J'ai pensé que l'accuracy était une mauvaise metrics pour evaluer les modeles NLP. Du au nombre de choix acceptable. Puis j'ai trouvé perplixity et top k accuracy.\n",
    "\n",
    "La perplexité est une mesure couramment utilisée pour évaluer les modèles de langage. Elle mesure à quel point le modèle est surpris par une séquence de mots. Une perplexité plus faible indique un meilleur modèle.\n",
    "\n",
    "Top k accuracy mesure le pourcentage de fois où le vrai mot suivant est parmi les K mots les plus probables prédits par le modèle.\n",
    "\n",
    "Ici, j'ai essayé de mettre en place la metrics perplixity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31812afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une métrique personnalisée pour la perplexité\n",
    "class PerplexityMetric(Callback):\n",
    "    def __init__(self, patience=5):\n",
    "        super(PerplexityMetric, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.best_weights = None\n",
    "        self.best_perplexity = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs.get('val_loss')\n",
    "        perplexity = 2 ** loss\n",
    "        print(f'\\nEpoch {epoch + 1}, Validation Perplexity: {perplexity:.4f}')\n",
    "\n",
    "        if perplexity < self.best_perplexity:\n",
    "            self.best_perplexity = perplexity\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                print(f'\\nRestoring model weights from the end of the best epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# Définition du modèle avec des couches supplémentaires et du Dropout\n",
    "model_drop = Sequential()\n",
    "model_drop.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model_drop.add(LSTM(150, return_sequences=True))  # LSTM avec retour des séquences\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(LSTM(100))  # Deuxième couche LSTM\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(Dense(128, activation='relu'))  # Couche Dense intermédiaire\n",
    "model_drop.add(Dropout(0.2))  # Dropout pour régularisation\n",
    "model_drop.add(Dense(total_words, activation='softmax'))  # Couche de sortie\n",
    "\n",
    "# Construire le modèle\n",
    "model_drop.build(input_shape=(None, max_sequence_len-1))\n",
    "\n",
    "# Afficher le résumé\n",
    "print(model_drop.summary())\n",
    "\n",
    "\n",
    "# Compilation du modèle\n",
    "model_drop.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Créer une instance de la métrique personnalisée pour la perplexité\n",
    "perplexity_metric = PerplexityMetric(patience=5)\n",
    "\n",
    "# Entraînement du modèle avec Early Stopping et la métrique de perplexité\n",
    "history_drop = model_drop.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[perplexity_metric]\n",
    ")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model_drop.save(\"modelLSMT_dropout.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb6deb",
   "metadata": {},
   "source": [
    "Bon on voit que l'entrainement du modèle ne dépasse pas les 8 ou 9 epochs même avec d'autres metrics que le loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bd5ed",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3d948-b5b5-4186-a628-86b0cf432fab",
   "metadata": {},
   "source": [
    "Les Transformers reposent entièrement sur des mécanismes d’attention (Self-Attention) au lieu d’une mémoire séquentielle. Cela permet d’apprendre les relations entre tous les mots d’une séquence en parallèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Charger le tokenizer du modèle préentraîné\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Lire le fichier\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Découper le texte en lignes puis mots\n",
    "lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence in lines:\n",
    "    words = sentence.strip().split()\n",
    "    if len(words) > 1:\n",
    "        X.append(' '.join(words[:-1]))  # tous sauf le dernier mot\n",
    "        y.append(words[-1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4509d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer et le modèle\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model_gpt2 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model_gpt2.eval()  # mode évaluation\n",
    "\n",
    "# Utiliser CUDA si dispo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_gpt2.to(device)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    input_text = X_test[i]\n",
    "    true_token = y_test[i]\n",
    "\n",
    "    # Tokenize l'entrée\n",
    "    input_ids = tokenizer_gpt2.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Générer 1 token après la phrase\n",
    "    with torch.no_grad():\n",
    "        output = model_gpt2.generate(input_ids, max_length=input_ids.shape[1] + 1, do_sample=False)\n",
    "\n",
    "    # Récupérer le dernier token généré\n",
    "    generated_token_id = output[0][-1].item()\n",
    "    generated_token = tokenizer_gpt2.decode([generated_token_id])\n",
    "\n",
    "    # Nettoyage simple du token attendu\n",
    "    expected = tokenizer_gpt2.convert_tokens_to_string([true_token]).strip()\n",
    "    predicted = generated_token.strip()\n",
    "\n",
    "    if expected == predicted:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "print(f\"Accuracy sur {total} exemples : {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2325c-4707-464d-aed4-a75443ab5e89",
   "metadata": {},
   "source": [
    "Donc, on a ici la meilleur accuracy mais il est testé sur moins de donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc2cbe-1010-4125-ae98-aaeea575b7ed",
   "metadata": {},
   "source": [
    "# Sauvegarde des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_lstm = load_model(\"modelLSMTsimple.h5\") # celui la il est pas mal (100 epochs)\n",
    "# Cela se sont arretés avant 10 epochs\n",
    "model_earlystopping = load_model(\"modelLSMTsimple_earlystopping.h5\") \n",
    "model_RNN = load_model(\"modelRNN.h5\")\n",
    "model_RNN_drop = load_model(\"modelRNN_dropout.h5\")\n",
    "model_lsmt_drop = load_model(\"modelLSMT_dropout.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb486634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed_text = \"My job is being\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predictions = model.predict(token_list, verbose=0)[0]\n",
    "    \n",
    "    # Obtenir les indices des 3 mots les plus probables\n",
    "    top_3_indices = np.argsort(predictions)[-3:][::-1]\n",
    "    \n",
    "    # Trouver les mots correspondants\n",
    "    top_3_words = [word for word, index in tokenizer.word_index.items() if index in top_3_indices]\n",
    "    \n",
    "    # Ajouter le mot le plus probable au texte\n",
    "    seed_text += \" \" + top_3_words[0]  # Utiliser le mot le plus probable\n",
    "    \n",
    "    print(f\"Top 3 mots possibles: {top_3_words}\")\n",
    "\n",
    "print(f\"Texte généré : {seed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e29479-2cb7-4d75-b259-cfe57a5da586",
   "metadata": {},
   "source": [
    "# Interface pour tester les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370bef7-0138-4ed0-a04d-8a1504c68c20",
   "metadata": {},
   "source": [
    "Les modeles ont été entrainer sur un texte anglais donc ils peuvent que predire si les mots avant sont anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21371182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fonction de prédiction\n",
    "def predict_next_words(model, tokenizer, max_sequence_len, seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    predictions = model.predict(token_list, verbose=0)[0]\n",
    "    \n",
    "    top_3_indices = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_words = [word for word, index in tokenizer.word_index.items() if index in top_3_indices]\n",
    "\n",
    "    return f\"Top 3 mots possibles : {top_3_words}\"\n",
    "\n",
    "def predict_next_words_gpt2(model, tokenizer, seed_text):\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='tf')\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "    predictions = outputs.logits[0, -1, :].numpy()\n",
    "    top_3_indices = np.argsort(predictions)[-3:][::-1]\n",
    "\n",
    "    # Décodage clair en supprimant les espaces inutiles\n",
    "    top_3_words = []\n",
    "    for idx in top_3_indices:\n",
    "        word = tokenizer.decode([idx]).strip()\n",
    "        # Filtrer les tokens bizarres\n",
    "        if word not in [\"\", \" \", \"\\n\"]:\n",
    "            top_3_words.append(word)\n",
    "\n",
    "    return f\"Top 3 mots possibles : {top_3_words}\"\n",
    "\n",
    "\n",
    "# Interface Gradio\n",
    "def interface(seed_text, model_name):\n",
    "    # Ici tu charges le bon modèle selon la sélection\n",
    "    if model_name == \"LSTM\":\n",
    "        model = model_lstm\n",
    "        tokenizer_used = tokenizer\n",
    "        max_seq_len = max_sequence_len\n",
    "        return predict_next_words(model, tokenizer_used, max_seq_len, seed_text)\n",
    "    elif model_name == \"RNN\":\n",
    "        model = model_RNN\n",
    "        tokenizer_used = tokenizer\n",
    "        max_seq_len = max_sequence_len\n",
    "        return predict_next_words(model, tokenizer_used, max_seq_len, seed_text)\n",
    "    elif model_name == \"DistilGPT2\":\n",
    "        return predict_next_words_gpt2(gpt2_model, gpt2_tokenizer, seed_text)\n",
    "    else:\n",
    "        return \"Modèle inconnu.\"\n",
    "\n",
    "# Chargement de DistilGPT2 pré-entraîné\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "gpt2_model = TFAutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "\n",
    "\n",
    "# Lancer l'interface\n",
    "models = [\"LSTM\", \"RNN\", \"DistilGPT2\"]\n",
    "interface_gradio = gr.Interface(\n",
    "    fn=interface,\n",
    "    inputs=[gr.Textbox(label=\"Votre phrase\"), gr.Dropdown(models, label=\"Choisissez un modèle\")],\n",
    "    outputs=gr.Textbox(label=\"Prédictions des mots\")\n",
    ")\n",
    "\n",
    "interface_gradio.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1b662",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "D'après les metrics, le transformer gpt2 et le modele lstm avec dropdown et early stopping sont les meilleurs. Mais, on regarde en testant les modèles que le modele que j'ai laissé s'entrainer pendant 100 epochs. Donne des meilleurs résultats que les autres modèles. A l'exception bien sur du modele pre entrainer. Cause algorithme plus performant et surtout entrainé sur un beaucoup plus gros dataset.\n",
    "\n",
    "Donc, je pense que la metrics d'évaluation n'est pas bon et vaux mieux laisser le modele s'entrainer beaucoup plus longtemps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
